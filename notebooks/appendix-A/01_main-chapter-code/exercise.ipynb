{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise A.1\n",
    "\n",
    "### Understanding Onehot Encoding and Cross Entropy in PyTorch\n",
    "\n",
    "#### What are Logits?\n",
    "\n",
    "**Logits** are the raw, unnormalized output scores produced by a neural network before applying an activation function like softmax. They are the direct result of the final linear layer in a classification model.\n",
    "\n",
    "Key characteristics of logits:\n",
    "- **Unnormalized**: Can be any real number (positive, negative, or zero)\n",
    "- **Not probabilities**: Don't sum to 1 and aren't bounded between 0 and 1\n",
    "- **Relative values matter**: Higher logits indicate stronger preference for that class\n",
    "- **Input to softmax**: Converting logits to probabilities requires the softmax function\n",
    "\n",
    "**Example:**"
   ],
   "id": "4dae58ab291fb363"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-20T19:14:03.131868Z",
     "start_time": "2025-10-20T19:14:03.130080Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "22bb1a32d78be842",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:19:58.013895Z",
     "start_time": "2025-10-20T19:19:58.009725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Logits from a neural network's final layer\n",
    "logits = torch.tensor([-0.3, -0.5, -0.5])  # Raw scores for 3 classes\n",
    "\n",
    "# Convert to probabilities using softmax\n",
    "probabilities = torch.softmax(logits, dim=0)  # [0.38, 0.31, 0.31]\n",
    "\n",
    "print(f\"logits: {logits}\")\n",
    "print(f\"probabilities: {probabilities}\")"
   ],
   "id": "ab5d99eb1b82750d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([-0.3000, -0.5000, -0.5000])\n",
      "probabilities: tensor([0.3792, 0.3104, 0.3104])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Onehot Encoding\n",
    "\n",
    "make this into markdown One-hot encoding transforms a class label $y \\in \\{0, 1, \\dots, k-1\\}$\n",
    "\n",
    "\n",
    "into a vector $\\mathbf{y}_{\\text{one-hot}} \\in \\{0, 1\\}^k$\n",
    "\n",
    "where:\n",
    "$$\n",
    "\\mathbf{y}_{\\text{one-hot}}[i] =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } i = y \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This allows us to compute:\n",
    "\n",
    "$$\\text{Loss} = - \\sum_{i=1}^{k} y_i \\cdot \\log(\\hat{y}_i)$$\n",
    "\n",
    "Where $y_i$ is the one-hot, and $\\hat{y}_i$ is the predicted probability."
   ],
   "id": "84a09aa7887b1b1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:38:03.249533Z",
     "start_time": "2025-10-20T19:38:03.245173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def to_onehot(t: Tensor, num_classes: int):\n",
    "    \"\"\"\n",
    "    Converts a tensor of class indices to a one-hot encoded tensor.\n",
    "    \n",
    "    One-hot encoding is a representation where each class index is converted to a binary vector\n",
    "    with a length equal to the number of classes. The vector has a value of 1.0 at the index\n",
    "    corresponding to the class and 0.0 at all other positions.\n",
    "    \n",
    "    How it works:\n",
    "    1. Creates a zero tensor of shape (N, num_classes) where N is the number of samples\n",
    "    2. Uses scatter_ to place 1.0 at the column index corresponding to each class\n",
    "    3. Converts the result to float type\n",
    "    \n",
    "    Args:\n",
    "        t: Tensor of shape (N,) containing class indices (integers from 0 to num_classes-1)\n",
    "        num_classes: The total number of classes to encode\n",
    "    \n",
    "    Returns:\n",
    "        A one-hot encoded tensor of shape (N, num_classes) where each row is a binary vector\n",
    "    \n",
    "    Example:\n",
    "        >>> y = torch.tensor([0, 1, 2, 2])\n",
    "        >>> y_onehot = to_onehot(y, 3)\n",
    "        >>> print(y_onehot)\n",
    "        tensor([[1., 0., 0.],  # class 0: one-hot encoded as [1, 0, 0]\n",
    "                [0., 1., 0.],  # class 1: one-hot encoded as [0, 1, 0]\n",
    "                [0., 0., 1.],  # class 2: one-hot encoded as [0, 0, 1]\n",
    "                [0., 0., 1.]]) # class 2: one-hot encoded as [0, 0, 1]\n",
    "        \n",
    "        In this example:\n",
    "        - Input has 4 samples with class indices [0, 1, 2, 2]\n",
    "        - Output is a 4x3 matrix (4 samples, 3 classes)\n",
    "        - Each row represents one sample's class as a binary vector\n",
    "        - The position of the 1.0 indicates which class that sample belongs to\n",
    "    \"\"\"\n",
    "    y_onehot = torch.zeros(t.size(0), num_classes)\n",
    "    y_onehot.scatter_(1, t.view(-1, 1).long(), 1).float()\n",
    "    return y_onehot\n",
    "\n",
    "\n",
    "y: Tensor = torch.tensor([0, 1, 2, 2])\n",
    "\n",
    "y_enc = to_onehot(y, 3)\n",
    "\n",
    "print('one-hot encoding:\\n', y_enc)\n"
   ],
   "id": "1bdf54dc4079c07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoding:\n",
      " tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Softmax\n",
    "\n",
    "Suppose we have some net inputs Z, where each row is one training example:"
   ],
   "id": "1d4f1e61e34655b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:22:58.313513Z",
     "start_time": "2025-10-20T19:22:58.309189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Z is a tensor containing the raw output scores (logits) from a neural network layer\n",
    "# before applying the softmax activation function.\n",
    "#\n",
    "# Shape: (4, 3) - representing 4 training examples with 3 possible classes each.\n",
    "# Each row represents one training example's raw scores across all classes\n",
    "# Each column represents a particular class (class 0, class 1, class 2)\n",
    "#\n",
    "# Example interpretation:\n",
    "# - First sample:  [-0.3, -0.5, -0.5]  → slightly prefers class 0 (least negative)\n",
    "# - Second sample: [-0.4, -0.1, -0.5]  → strongly prefers class 1 (least negative)\n",
    "# - Third sample:  [-0.3, -0.94, -0.5] → prefers class 0\n",
    "# - Fourth sample: [-0.99, -0.88, -0.5] → prefers class 2\n",
    "#\n",
    "# These logits will be converted to probabilities using the softmax function below,\n",
    "# which transforms them into a probability distribution where all values are between \n",
    "# 0 and 1 and sum to 1 for each row.\n",
    "Z: Tensor = torch.tensor([[-0.3, -0.5, -0.5],\n",
    "                          [-0.4, -0.1, -0.5],\n",
    "                          [-0.3, -0.94, -0.5],\n",
    "                          [-0.99, -0.88, -0.5]])\n",
    "\n",
    "Z\n"
   ],
   "id": "f5366b02684e1252",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3000, -0.5000, -0.5000],\n",
       "        [-0.4000, -0.1000, -0.5000],\n",
       "        [-0.3000, -0.9400, -0.5000],\n",
       "        [-0.9900, -0.8800, -0.5000]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we convert them to `probabilities` via softmax:\n",
    "\n",
    "$$P(y=j \\mid z^{(i)}) = \\sigma_{\\text{softmax}}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=0}^{k} e^{z_{k}^{(i)}}}.$$\n",
    "\n",
    "where: \n",
    "* $z^{(i)}$ is the $i$th row of input tensor $z$\n",
    "* $j$ is the class label index\n",
    "* $k$ is the number of classes"
   ],
   "id": "ea4b3f697fe30972"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:14:03.188993Z",
     "start_time": "2025-10-20T19:14:03.186430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(z: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Applies the softmax function to convert logits (raw scores) into probabilities.\n",
    "    \n",
    "    The softmax function transforms each row of input tensor z into a probability distribution\n",
    "    where all values are between 0 and 1 and sum to 1. This is commonly used in multi-class\n",
    "    classification to convert model outputs into class probabilities.\n",
    "    \n",
    "    Formula: softmax(z_i) = exp(z_i) / sum(exp(z_j)) for all j\n",
    "    \n",
    "    Args:\n",
    "        z: Input tensor of shape (N, C) where N is batch size and C is the number of classes\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of the same shape as input with probability distributions across each row.\n",
    "        Each row sums to 1.0 and all values are in range [0, 1].\n",
    "    \n",
    "    Example:\n",
    "        Input: [[-0.3, -0.5, -0.5]]\n",
    "        Output: [[0.3792, 0.3104, 0.3104]] # probabilities summing to 1.0\n",
    "    \"\"\"\n",
    "    return (torch.exp(z.t()) / torch.sum(torch.exp(z), dim=1)).t()\n",
    "\n",
    "\n",
    "smax = softmax(Z)\n",
    "print('softmax:\\n', smax)\n"
   ],
   "id": "add04ae2266495b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax:\n",
      " tensor([[0.3792, 0.3104, 0.3104],\n",
      "        [0.3072, 0.4147, 0.2780],\n",
      "        [0.4263, 0.2248, 0.3490],\n",
      "        [0.2668, 0.2978, 0.4354]])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The probabilities can then be converted back to class labels based on the largest probability in each row:",
   "id": "6fdeccc9c4cdc5d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:14:03.222903Z",
     "start_time": "2025-10-20T19:14:03.220370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_classlabel(z: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Converts probability distributions or one-hot encoded tensors to class labels.\n",
    "    \n",
    "    This function takes a tensor where each row represents either a probability distribution\n",
    "    (from softmax) or a one-hot encoded vector and returns the index of the maximum value\n",
    "    in each row, which corresponds to the predicted or actual class label.\n",
    "    \n",
    "    Args:\n",
    "        z: Input tensor of shape (N, C) where N is the number of samples and C is the number\n",
    "           of classes. Each row should contain either probabilities or one-hot encoded values.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape (N,) containing the class label indices (0 to C-1) for each sample.\n",
    "    \n",
    "    Example:\n",
    "        Input (probabilities from softmax):\n",
    "        tensor([[0.1, 0.7, 0.2], # the highest probability at index 1\n",
    "                [0.6, 0.3, 0.1]]) # the highest probability at index 0\n",
    "        \n",
    "        Output:\n",
    "        tensor([1, 0])  # class labels\n",
    "        \n",
    "        Input (one-hot encoded):\n",
    "        tensor([[1., 0., 0.],    # one-hot encoded class 0\n",
    "                [0., 0., 1.]])   # one-hot encoded class 2\n",
    "        \n",
    "        Output:\n",
    "        tensor([0, 2]) # class labels\n",
    "    \"\"\"\n",
    "    return torch.argmax(z, dim=1)\n",
    "\n",
    "\n",
    "print('predicted class labels: ', to_classlabel(smax))\n",
    "print('true class labels: ', to_classlabel(y_enc))\n"
   ],
   "id": "25929cdd1101246a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class labels:  tensor([0, 1, 0, 2])\n",
      "true class labels:  tensor([0, 1, 2, 2])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cross Entropy\n",
    "\n",
    "Cross-entropy is a loss function that measures the difference between two probability distributions: the true distribution (actual class labels) and the predicted distribution (model's output probabilities). It is widely used in classification tasks because it effectively quantifies how well the model's predictions match the true labels.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For each training example, the cross-entropy loss is computed as:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H(T_i, O_i),$$\n",
    "\n",
    "where: \n",
    "* $T_i$ is the true class label (one-hot encoded) for the $i$th training example\n",
    "* $O_i$ is the predicted probability distribution (from softmax) for the $i$th training example\n",
    "* $n$ is the number of training examples \n",
    "* $H$ is the cross-entropy function\n",
    "\n",
    "The cross-entropy function for a single example is:\n",
    "\n",
    "$$H(T_i, O_i) = -\\sum_m T_{i,m} \\cdot \\log(O_{i,m}),$$\n",
    "\n",
    "where $m$ is the class label index ranging over all classes.\n",
    "\n",
    "### Why Cross-Entropy is Used\n",
    "\n",
    "1. **Probabilistic Interpretation**: Cross-entropy naturally measures the \"distance\" between probability distributions, making it ideal for classification where we predict class probabilities.\n",
    "\n",
    "2. **Penalizes Confident Wrong Predictions**: The logarithm heavily penalizes predictions that are confidently wrong. For example, if the true class has a predicted probability of 0.01, the loss is much higher than if it were 0.5.\n",
    "\n",
    "3. **Smooth Gradients**: Unlike accuracy (which is discrete), cross-entropy provides smooth gradients that enable effective gradient-based optimization during training.\n",
    "\n",
    "4. **Works Well with Softmax**: When combined with softmax activation, cross-entropy loss has mathematically convenient properties for backpropagation and optimization.\n",
    "\n",
    "5. **Encourages Calibrated Probabilities**: The loss pushes the model to output well-calibrated probability distributions, not just correct class predictions.\n"
   ],
   "id": "5e27503716fcc1db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cross_entropy(softmax: Tensor, y_target: Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss between predicted probabilities and target labels.\n",
    "    \n",
    "    Cross-entropy is a loss function commonly used in multi-class classification tasks.\n",
    "    It measures the difference between the predicted probability distribution (from softmax)\n",
    "    and the true distribution (one-hot encoded target labels).\n",
    "    \n",
    "    Formula: H(T, O) = -sum(T * log(O))\n",
    "    where:\n",
    "    - T is the true class label (one-hot encoded)\n",
    "    - O is the predicted probability distribution (from softmax)\n",
    "    \n",
    "    The function computes the negative sum of element-wise pof the log of predicted\n",
    "    probabilities and the true labels. Lower cross-entropy values indicate better predictions.\n",
    "    \n",
    "    Args:\n",
    "        softmax: Tensor of shape (N, C) containing predicted probabilities for each class,\n",
    "                 where N is the number of samples and C is the number of classes.\n",
    "                 Values should be in range [0, 1] and each row should sum to 1.\n",
    "        y_target: Tensor of shape (N, C) containing one-hot encoded true class labels.\n",
    "                  Each row has a 1.0 at the true class index and 0.0 elsewhere.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape (N,) containing the cross-entropy loss for each sample.\n",
    "    \n",
    "    Example:\n",
    "        >>> # Predicted probabilities (after softmax)\n",
    "        >>> softmax = torch.tensor([[0.3792, 0.3104, 0.3104],  # predicts class 0\n",
    "        ...                         [0.3072, 0.4147, 0.2780],  # predicts class 1\n",
    "        ...                         [0.4263, 0.2248, 0.3490],  # predicts class 0\n",
    "        ...                         [0.2668, 0.2978, 0.4354]]) # predicts class 2\n",
    "        >>> \n",
    "        >>> # True labels (one-hot encoded)\n",
    "        >>> y_target = torch.tensor([[1., 0., 0.],  # true class 0\n",
    "        ...                          [0., 1., 0.],  # true class 1\n",
    "        ...                          [0., 0., 1.],  # true class 2\n",
    "        ...                          [0., 0., 1.]]) # true class 2\n",
    "        >>> \n",
    "        >>> loss = cross_entropy(softmax, y_target)\n",
    "        >>> print(loss)\n",
    "        tensor([0.9698, 0.8796, 1.0520, 0.8316])\n",
    "        \n",
    "        In this example:\n",
    "        - First sample: correct prediction (class 0), low loss (0.9698)\n",
    "        - Second sample: correct prediction (class 1), low loss (0.8796)\n",
    "        - Third sample: incorrect prediction (predicted 0, true 2), higher loss (1.0520)\n",
    "        - Fourth sample: correct prediction (class 2), low loss (0.8316)\n",
    "    \"\"\"\n",
    "    return - torch.sum(torch.log(softmax) * y_target, dim=1)\n",
    "\n",
    "\n",
    "xent = cross_entropy(smax, y_enc)\n",
    "print('Cross Entropy:', xent)\n"
   ],
   "id": "6c1b3e6223c69b7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## In PyTorch",
   "id": "572050c4aef911e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:14:03.249105Z",
     "start_time": "2025-10-20T19:14:03.247341Z"
    }
   },
   "cell_type": "code",
   "source": "import torch.nn.functional as F",
   "id": "a88e9989f6b577a1",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that `nll_loss` takes log(softmax) as input:",
   "id": "a82351c2f6cd25f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:14:03.258660Z",
     "start_time": "2025-10-20T19:14:03.256016Z"
    }
   },
   "cell_type": "code",
   "source": "F.nll_loss(torch.log(smax), y, reduction='none')",
   "id": "32e93b09d34cc050",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9698, 0.8801, 1.0527, 0.8314])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that `cross_entropy` takes logits as input:",
   "id": "1f4b45d05489d094"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:14:03.269415Z",
     "start_time": "2025-10-20T19:14:03.266511Z"
    }
   },
   "cell_type": "code",
   "source": "F.cross_entropy(Z, y, reduction='none')",
   "id": "69282ea27e6798fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9698, 0.8801, 1.0527, 0.8314])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Defaults\n",
    "By default, nll_loss & cross_entropy are already returning the average over training examples, which is useful for stability during optimization."
   ],
   "id": "5dc1f4d4bbbb9e27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:14:27.501203Z",
     "start_time": "2025-10-20T19:14:27.498390Z"
    }
   },
   "cell_type": "code",
   "source": "F.cross_entropy(Z, y)",
   "id": "a6e265c74a12cea5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9335)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:14:28.762405Z",
     "start_time": "2025-10-20T19:14:28.758763Z"
    }
   },
   "cell_type": "code",
   "source": "torch.mean(cross_entropy(smax, y_enc))",
   "id": "44372aeb1becc158",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9335)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
